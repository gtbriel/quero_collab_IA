{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim.downloader as api\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/hevelyncarvalho/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/hevelyncarvalho/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/hevelyncarvalho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([ \n",
    "      'Comecei um novo projeto em Rails e estou adorando!!',\n",
    "      'Nossa, muito chato trabalhar com Rails! Prefiro codar em Python... Muito mais facil.',\n",
    "      'Estou querendo migrar para o time de Data Engineering, quero começar a trabalhar com Airflow',\n",
    "      'Airflow é uma tecnologia muito inovadora, automatiza diversos processos'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    stopWords = set(stopwords.words('portuguese'))\n",
    "    words = [WordNetLemmatizer().lemmatize(w) for w in tokens if (w.isalpha() and w not in stopWords)]\n",
    "    return words\n",
    "\n",
    "l = list((map(preprocessing, data))) # pre-processed dataset\n",
    "idx = np.array([index for index,a in enumerate(l)])  #  if len(a) > 5\n",
    "docs_pp_tokens = np.array(l)[idx]  # sem documentos vazios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['comecei', 'novo', 'projeto', 'rail', 'adorando'], tags=[0]),\n",
       " TaggedDocument(words=['chato', 'trabalhar', 'rail', 'prefiro', 'codar', 'python', 'facil'], tags=[1]),\n",
       " TaggedDocument(words=['querendo', 'migrar', 'time', 'data', 'engineering', 'quero', 'começar', 'trabalhar', 'airflow'], tags=[2]),\n",
       " TaggedDocument(words=['airflow', 'tecnologia', 'inovadora', 'automatiza', 'diversos', 'processos'], tags=[3])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggedDocs = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs_pp_tokens)] #matriz de documentos com as tags, cada linha é um vetor dos tokens de cada doc\n",
    "taggedDocs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "d2v = Doc2Vec(dm=0, vector_size=50, window=2, min_count=1, sample=6e-5, alpha=0.03, min_alpha=0.0007)\n",
    "d2v.build_vocab(documents=taggedDocs)\n",
    "total = d2v.corpus_count\n",
    "#vocab = [TaggedDocument(w, [i]) for i, w in enumerate(nilc.vocab.keys())]\n",
    "#d2v.build_vocab(vocab, update=True)\n",
    "\n",
    "d2v.train(taggedDocs, total_examples=total, epochs=100)\n",
    "docs = np.array([d2v.docvecs[i] for i in range(len(taggedDocs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Estou usando muito Airflow ultimamente\"\n",
    "tokens = preprocessing(text)\n",
    "new_vector = d2v.infer_vector(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = np.array(d2v.docvecs.most_similar([new_vector]))\n",
    "idx = sims[:,0].astype(int)\n",
    "data[idx]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
